{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM y Métodos Kernel\n",
    "\n",
    "En este Notebook vamos a poner a prueba el algoritmo de SVMs, tanto en problemas de clasificación como de regresión. Adicionalmente, se analizará el funcionamiento de otros métodos que utilizan el concepto de [Kernel](https://en.wikipedia.org/wiki/Kernel_method). \n",
    "\n",
    "Contenidos:\n",
    "\n",
    "1. SVMs en problemas de clasificación\n",
    "    1. Ejemplos sintéticos\n",
    "    2. Ejemplo realista ([Pima Indian Diabetes dataset](https://www.kaggle.com/uciml/pima-indians-diabetes-database))\n",
    "    3. Recursive Feature Elimination (RFE)\n",
    "2. SVMs en problemas de regresión\n",
    "    1. Ejemplo sintético\n",
    "    2. Ejemplo realista ([House Sales in King County, USA](https://www.kaggle.com/harlfoxem/housesalesprediction))\n",
    "3. Otros métodos Kernel\n",
    "    1. Ridge Kernel Regression\n",
    "    2. Kernel PCA\n",
    "\n",
    "Lo primero es cargar las librerías y funciones necesarias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_decision_boundary_svm, CM_BRIGHT\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. SVMs en problemas de clasificación\n",
    "\n",
    "Comenzaremos analizando las máquinas de vectores soporte en clasificación, en ocasiones denominadas *Support Vector Classifiers*. Como hemos hecho en notebooks anteriores, primero probamos sobre ejemplos sintéticos:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Ejemplos sintéticos\n",
    "\n",
    "Cargamos y representamos nuestros ejemplos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ejemplo1\n",
    "ejemplo1 = pd.read_csv(\"./data/ex2data1.txt\", sep=\",\", header=None, names=['x1', 'x2','label'])\n",
    "\n",
    "# ejemplo2\n",
    "ejemplo2 = pd.read_csv(\"./data/ex2data2.txt\", sep=\",\", header=None, names=['x1', 'x2','label'])\n",
    "\n",
    "# ejemplo 3: Problema XOR \n",
    "np.random.seed(0)\n",
    "\n",
    "# -- parameters\n",
    "N     = 800\n",
    "mu    = 1.5      # Cambia este valor\n",
    "sigma = 1      # Cambia este valor\n",
    "\n",
    "# variables auxiliares\n",
    "unos = np.ones(int(N/4))\n",
    "random4 = sigma*np.random.randn(int(N/4),1)\n",
    "random2 = sigma*np.random.randn(int(N/2),1)\n",
    "\n",
    "# -- features\n",
    "y3 = np.concatenate([-1*unos,       unos,          unos,         -1*unos]) \n",
    "X1 = np.concatenate([-mu + random4, mu + random4, -mu + random4, mu + random4])\n",
    "X2 = np.concatenate([+mu + random2,               -mu + random2])\n",
    "X3 = np.hstack((X1,X2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "plt.scatter(ejemplo1['x1'], ejemplo1['x2'], c=ejemplo1['label'], cmap=CM_BRIGHT)\n",
    "plt.xlabel(\"$x_1$\", fontsize=16)\n",
    "plt.ylabel(\"$x_2$\", fontsize=16)\n",
    "plt.title('Ejemplo 1')\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.scatter(ejemplo2['x1'], ejemplo2['x2'], c=ejemplo2['label'], cmap=CM_BRIGHT)\n",
    "plt.xlabel(\"$x_1$\", fontsize=16)\n",
    "plt.ylabel(\"$x_2$\", fontsize=16)\n",
    "plt.title('Ejemplo 2')\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.scatter(X3[:,0], X3[:,1], c=y3, cmap=CM_BRIGHT)\n",
    "plt.xlabel(\"$x_1$\", fontsize=16)\n",
    "plt.ylabel(\"$x_2$\", fontsize=16)\n",
    "plt.title('Ejemplo 3')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Ejemplo 1\n",
    "# preparamos los datos\n",
    "data1 = ejemplo1.values\n",
    "X1 = data1[:,0:2]\n",
    "y1 = data1[:,-1]\n",
    "\n",
    "# creamos el modelo y ajustamos\n",
    "svmModel1= SVC(kernel='linear', probability = True)\n",
    "svmModel1.fit(X1,y1)\n",
    "\n",
    "plot_decision_boundary_svm(X1, y1, svmModel1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Donde hemos resaltado los **vectores soporte**, la **frontera de separación** y el **margen**. \n",
    "\n",
    "Como el problema no es linealmente separable, observamos errores de clasificación que caen tanto dentro del margen, como al otro lado de la frontera de separación correspondiente. \n",
    "\n",
    "Sabemos que podemos permitir ciertos errores dentro del margen, y los penalizamos con un coste $C$. \n",
    "\n",
    "* Si tenemos un valor elevado del coste $C$, estaremos penalizando mucho los errores, y por tanto se obtienen fronteras más ajustadas (mayor complejidad, mayor riesgo de overfitting, potenciales mejores prestaciones). \n",
    "\n",
    "* De otro lado, si tenemos un valor pequeño del coste $C$, no daremos mucha importancia a los errores, y por tanto se obtienen fronteras menos ajustadas (menor complejidad, menor riesgo de overfitting, potenciales peores prestaciones). \n",
    "\n",
    "Se puede modificar el coste $C$ mediante el parámetro de mismo nombre en scikit-learn. Por defecto, $C=1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svmModel1= SVC(kernel='linear',probability=True,C = 0.0001) # PIENSA ANTES COMO SERÁ EL RESULTADO!\n",
    "svmModel1.fit(X1, y1)\n",
    "\n",
    "plot_decision_boundary_svm(X1, y1, svmModel1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svmModel1= SVC(kernel='linear',probability=True,C = 1e5) # PIENSA ANTES COMO SERÁ EL RESULTADO!\n",
    "svmModel1.fit(X1, y1)\n",
    "\n",
    "plot_decision_boundary_svm(X1, y1, svmModel1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplos 2 y 3\n",
    "\n",
    "Comenzamos por el ejemplo 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparamos los datos\n",
    "data2 = ejemplo2.values\n",
    "X2 = data2[:,0:2]\n",
    "y2 = data2[:,-1]\n",
    "\n",
    "# creamos el modelo\n",
    "svmModel2= SVC(kernel='linear',probability=True)\n",
    "svmModel2.fit(X2,y2)\n",
    "\n",
    "plot_decision_boundary_svm(X2,y2,0.05,svmModel2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos comprobar, un kernel lineal no nos sirve para generar una frontera de separación no lineal, así que tenemos que utilizar otros kernels.\n",
    "\n",
    "Vamos a volver a teoría un rato, para saber cómo entrenar un algoritmo <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\">SVC</a> con diferentes Kernels.\n",
    "\n",
    "<div class = \"alert alert-success\">\n",
    "EJERCICIO 7.1: Entrena un algoritmo SVM dobre el ejemplo 2 con kernel RBF y con kernel polinómico.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RBF\n",
    "data2 = ejemplo2.values\n",
    "X2 = ...\n",
    "y2 = ...\n",
    "\n",
    "svmModel2 = ...\n",
    "\n",
    "plot_decision_boundary_svm(X2, y2, svmModel2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polinómico\n",
    "data2 = ejemplo2.values\n",
    "X2 = ...\n",
    "y2 = ...\n",
    "\n",
    "svmModel2 = ...\n",
    "\n",
    "plot_decision_boundary_svm(X2, y2, svmModel2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\">\n",
    "EJERCICIO 7.2: Entrena una SVM sobre el ejemplo 3.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo 3\n",
    "svmModel3 = ...\n",
    "\n",
    "plot_decision_boundary_svm(X3,y3,0.05,svmModel3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\">\n",
    "EJERCICIO 7.3: Suponiendo un kernel RBF, calcule el valor óptimo de <b>C</b> y <b>gamma</b> para el ejemplo 3, ¿cuáles son las prestaciones del algoritmo para este ejemplo?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# ... código aquí\n",
    "# Paso 1:\n",
    "X_train, X_test, y_train, y_test = ...\n",
    "\n",
    "# Paso 2:\n",
    "vectorC = np.logspace(-3, 3, 21)\n",
    "vectorG = np.logspace(-5, 1, 21)\n",
    "\n",
    "param_grid = ...\n",
    "\n",
    "# Paso 3:\n",
    "grid = ...\n",
    "\n",
    "print(\"best mean cross-validation score: {:.3f}\".format(grid.best_score_))\n",
    "print(\"best parameters: {}\".format(grid.best_params_))\n",
    "\n",
    "# Mostramos prestaciones en CV\n",
    "scores = grid.cv_results_['mean_test_score'].reshape(len(vectorC),len(vectorG))\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.imshow(scores, interpolation='nearest', vmin= 0.6, vmax=0.9)\n",
    "plt.xlabel('log(gamma)')\n",
    "plt.ylabel('log(C)')\n",
    "plt.colorbar()\n",
    "plt.xticks(np.arange(len(vectorG)), np.log10(vectorG), rotation=90)\n",
    "plt.yticks(np.arange(len(vectorC)), np.log10(vectorC))\n",
    "plt.title('5-fold accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nota: para ver cuáles serían los valores reales de C y gamma, hay que deshacer el logaritmo:\n",
    "Copt = grid.best_params_['C']\n",
    "Gopt = grid.best_params_['gamma']\n",
    "\n",
    "print(np.log10(Copt))\n",
    "print(np.log10(Gopt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mostramos prestaciones en test\n",
    "Copt = grid.best_params_['C']\n",
    "Gopt = grid.best_params_['gamma']\n",
    "\n",
    "svmModel3 = SVC(kernel='rbf',gamma = Gopt, C = Copt, probability=True).fit(X_train,y_train)\n",
    "\n",
    "print(f'Acc (TEST):{svmModel3.score(X_test,y_test):.2f}')\n",
    "\n",
    "plot_decision_boundary_svm(X_test, y_test, svmModel3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Ejemplo realista\n",
    "\n",
    "Vamos a utilizar el mismo conjunto de datos del Notebook anterior, el [Pima Indian Diabetes dataset](https://www.kaggle.com/uciml/pima-indians-diabetes-database)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cargamos datos\n",
    "data = pd.read_csv('./data/diabetes.csv',sep=',', decimal='.')\n",
    "\n",
    "# preparamos los datos\n",
    "features = data.columns.drop(['Outcome'])\n",
    "X = data[features].values\n",
    "y = data['Outcome'].values\n",
    "\n",
    "print('Dimensionalidad datos: ', X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\">\n",
    "EJERCICIO 7.4: Ajusta un modelo de SVM al conjunto de datos anterior. Devuelve las prestaciones en el conjunto de test. No hace falta realizar el análisis exploratorio ni nada similar.\n",
    "</div>\n",
    "\n",
    "NOTA: Best Test ACC (lo esperado) = 0.794  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paso 1\n",
    "X_train, X_test, y_train, y_test = ...\n",
    "\n",
    "print('Dimensionalidad train: ', X_train.shape)\n",
    "print('Dimensionalidad test: ', X_test.shape)\n",
    "\n",
    "# Paso 2\n",
    "vectorC = np.logspace(2, 8, 10)\n",
    "vectorG = np.logspace(-15, -4, 10)\n",
    "\n",
    "param_grid = ...\n",
    "\n",
    "grid = ...\n",
    "\n",
    "print(\"best mean cross-validation score: {:.3f}\".format(grid.best_score_))\n",
    "print(\"best parameters: {}\".format(grid.best_params_))\n",
    "print(\"Gamma real: \", np.log10(grid.best_params_['gamma']))\n",
    "print(\"C real: \", np.log10(grid.best_params_['C']))\n",
    "\n",
    "# Mostramos prestaciones en CV\n",
    "scores = grid.cv_results_['mean_test_score'].reshape(len(vectorC),len(vectorG))\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.imshow(scores, interpolation='nearest', vmin= 0.5, vmax=0.77)\n",
    "plt.xlabel('log(gamma)')\n",
    "plt.ylabel('log(C)')\n",
    "plt.colorbar()\n",
    "plt.xticks(np.arange(len(vectorG)), np.log10(vectorG), rotation=90)\n",
    "plt.yticks(np.arange(len(vectorC)), np.log10(vectorC))\n",
    "plt.title('5-fold accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... código aquí: prestaciones en test\n",
    "svm = ...\n",
    "\n",
    "print(f'Acc (TEST):{svm.score(X_test, y_test):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Recursive Feature Elimination\n",
    "\n",
    "Sobre el conjunto anterior, vamos a implementar el algoritmo de selección de características [RFE con validación cruzada](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html#sklearn.feature_selection.RFECV), para que casi automáticamente podamos abordar una selección como la realizada en el Notebook 5, sección 1.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "svc = SVC(kernel='linear') # ¡sólo funciona con Kernel Lineal!\n",
    "\n",
    "rfecv = RFECV(estimator=svc, step=1, cv=5, scoring='accuracy')\n",
    "rfecv.fit(X_train, y_train)\n",
    "\n",
    "print('Variables seleccionadas: ',[f for f in features[rfecv.support_]] )\n",
    "print('Acc (TEST): %0.2f'%rfecv.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking = dict(zip(features, rfecv.ranking_))\n",
    "for k, v in ranking.items():\n",
    "    print(f'{k}: {v}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "nfeaturesOptimo = rfecv.n_features_\n",
    "rfe = RFE(estimator=svc, step=1, n_features_to_select= 4).fit(X_train,y_train)\n",
    "print('Variables seleccionadas: ',[f for f in features[rfe.support_]] )\n",
    "\n",
    "print(f'Acc (TEST): {rfe.score(X_test,y_test):0.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. SVMs en problemas de regresión\n",
    "\n",
    "El algoritmo SVM en regresión se denomina [SVR](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html).\n",
    "\n",
    "## 2.1 Ejemplo sintético"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_train = 10\n",
    "N_test  = 100\n",
    "\n",
    "# función verdadera g(x)\n",
    "x = np.linspace(0,1,N_test)\n",
    "g_x = np.cos(1.5*np.pi*x)\n",
    "\n",
    "# proceso y\n",
    "np.random.seed(0) # para asegurar reproducibilidad\n",
    "epsilon = np.random.randn(N_test) * 0.2\n",
    "y = g_x + epsilon\n",
    "\n",
    "# Datos: D = {x_i,y_i}, obtenemos una muestra\n",
    "idx = np.random.randint(0,N_test,N_train)\n",
    "x_i = x[idx]\n",
    "y_i = y[idx]\n",
    "\n",
    "plt.plot(x,g_x,'r',label='g(x)')\n",
    "plt.plot(x_i,y_i,'b.',label='y')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\">\n",
    "EJERCICIO 7.5: Ajusta un modelo SVR para el ejemplo anterior, usando kernel RBF y C = 0.5\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "# datos\n",
    "X_train = x_i.reshape(-1, 1)\n",
    "y_train = y_i\n",
    "X_test  = x.reshape(-1, 1)\n",
    "\n",
    "# ... código aquí\n",
    "svr = ...\n",
    "\n",
    "# predicción\n",
    "y_hat = svr.predict(X_test)\n",
    "\n",
    "# error test\n",
    "error_test = np.mean(np.power(y - y_hat,2)) \n",
    "\n",
    "# representamos\n",
    "plt.plot(x,g_x,'r',label='$y$')\n",
    "plt.plot(x_i,y_i,'b.',label='$y_i$')\n",
    "plt.plot(x,y_hat,'g',label='$\\hat{y}$')\n",
    "plt.title('MSE:%.2f'%error_test)\n",
    "plt.legend()\n",
    "plt.xlim((0, 1))\n",
    "plt.ylim((-2, 2))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Ejemplo realista en regresión\n",
    "\n",
    "Volvemos a nuestro conjunto de datos ya conocido:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cargamos datos\n",
    "house_data = pd.read_csv(\"./data/kc_house_data.csv\") # cargamos fichero\n",
    "\n",
    "# Eliminamos las columnas id y date \n",
    "house_data = house_data.drop(['id','date'], axis=1)\n",
    "\n",
    "# convertir las variables en pies al cuadrado en metros al cuadrado \n",
    "feetFeatures = ['sqft_living','sqft_lot','sqft_above','sqft_basement','sqft_living15','sqft_lot15']\n",
    "house_data[feetFeatures] = house_data[feetFeatures].apply(lambda x: x * 0.3048 * 0.3048)\n",
    "\n",
    "# renombramos\n",
    "house_data.columns = ['price','bedrooms','bathrooms','sqm_living','sqm_lot','floors','waterfront','view','condition',\n",
    "                      'grade','sqm_above','sqm_basement','yr_built','yr_renovated','zip_code','lat','long',\n",
    "                      'sqm_living15','sqm_lot15']\n",
    "\n",
    "# convertimos el DataFrame al formato necesario para scikit-learn\n",
    "data = house_data.values \n",
    "\n",
    "y = data[:,0]     # nos quedamos con la 1ª columna, price\n",
    "X = data[:,1:]      # nos quedamos con el resto\n",
    "\n",
    "feature_names = house_data.columns[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# paso 1\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, np.log10(y), test_size=.25, random_state = 2)\n",
    "\n",
    "print('Datos entrenamiento: ', X_train.shape)\n",
    "print('Datos test: ', X_test.shape)\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "XtrainScaled = scaler.transform(X_train)\n",
    "XtestScaled  = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente celda tarda en ejecutarse un tiempo (del orden de 1 hora)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "# Paso 2:\n",
    "vectorC = np.logspace(-2, 2, 10)\n",
    "vectorG = np.logspace(-5, 1, 8)\n",
    "\n",
    "param_grid = {'C': vectorC, 'gamma':vectorG}\n",
    "grid = GridSearchCV(SVR(kernel='rbf'), param_grid=param_grid, cv = 5, verbose=2)\n",
    "grid.fit(XtrainScaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"best mean cross-validation score: {:.3f}\".format(grid.best_score_))\n",
    "print(\"best parameters: {}\".format(grid.best_params_))\n",
    "\n",
    "print(\"Gamma en la gráfica: \", np.log10(grid.best_params_['gamma']))\n",
    "print(\"C en la gráfica: \", np.log10(grid.best_params_['C']))\n",
    "\n",
    "# Mostramos prestaciones en CV\n",
    "scores = grid.cv_results_['mean_test_score'].reshape(len(vectorC),len(vectorG))\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.imshow(scores, interpolation='nearest', vmin= 0.6, vmax=0.9)\n",
    "plt.xlabel('log(gamma)')\n",
    "plt.ylabel('log(C)')\n",
    "plt.colorbar()\n",
    "plt.xticks(np.arange(len(vectorG)), np.log10(vectorG), rotation=90)\n",
    "plt.yticks(np.arange(len(vectorC)), np.log10(vectorC))\n",
    "plt.title('5-fold accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 3:\n",
    "Copt = grid.best_params_['C']\n",
    "Gopt = grid.best_params_['gamma']\n",
    "\n",
    "svmModel = SVR(kernel='rbf',gamma = Gopt, C = Copt).fit(XtrainScaled,y_train)\n",
    "print(f'Acc (TEST): {svmModel.score(XtestScaled,y_test):0.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este problema es suficientemente complejo como para ser analizado en una [tesis de máster](http://scholarworks.sjsu.edu/cgi/viewcontent.cgi?article=1540&context=etd_projects)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Otros métodos Kernel\n",
    "\n",
    "## 3.1 Kernel Ridge Regression\n",
    "\n",
    "Vemos un ejemplo sencilo de [Kernel Ridge Regression](http://scikit-learn.org/stable/modules/generated/sklearn.kernel_ridge.KernelRidge.html#sklearn.kernel_ridge.KernelRidge). En este caso no es necesario que utilicemos un modelo de datos de alta dimensionalidad, basta elegir adecuadamente los parámetros libres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_train = 10\n",
    "N_test  = 100\n",
    "\n",
    "# función verdadera g(x)\n",
    "x = np.linspace(0,1,N_test)\n",
    "g_x = np.cos(1.5*np.pi*x)\n",
    "\n",
    "# proceso y\n",
    "np.random.seed(0) # para asegurar reproducibilidad\n",
    "epsilon = np.random.randn(N_test) * 0.2\n",
    "y = g_x + epsilon\n",
    "\n",
    "# Datos: D = {x_i,y_i}, obtenemos una muestra\n",
    "idx = np.random.randint(0,N_test,N_train)\n",
    "x_i = x[idx]\n",
    "y_i = y[idx]\n",
    "\n",
    "plt.plot(x,g_x,'r',label='g(x)')\n",
    "plt.plot(x_i,y_i,'b.',label='y')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.kernel_ridge import KernelRidge\n",
    "\n",
    "# datos\n",
    "X_train = x_i.reshape(-1, 1)\n",
    "y_train = y_i\n",
    "X_test  = x.reshape(-1, 1)\n",
    "\n",
    "# definimos modelo\n",
    "kRidge = KernelRidge(kernel='rbf',gamma=10,alpha=0.1)\n",
    "kRidge.fit(X_train,y_train)\n",
    "\n",
    "# predicción\n",
    "y_hat = kRidge.predict(X_test)\n",
    "\n",
    "# error test\n",
    "error_test = np.mean(np.power(y - y_hat,2)) \n",
    "\n",
    "# representamos\n",
    "plt.plot(x,g_x,'r',label='$y$')\n",
    "plt.plot(x_i,y_i,'b.',label='$y_i$')\n",
    "plt.plot(x,y_hat,'g',label='$\\hat{y}$')\n",
    "plt.title('MSE:%.2f'%error_test)\n",
    "plt.legend()\n",
    "plt.xlim((0, 1))\n",
    "plt.ylim((-2, 2))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Kernel PCA\n",
    "\n",
    "[Kernel PCA](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.143.2441&rep=rep1&type=pdf) es una reformulación del algoritmo PCA utilizando un kernel, lo que permite aplicar PCA en un espacio de alta dimensionalidad. En vez de calcular los autovectores de la matriz de covarianza, estos se calculan a partir de la matriz de kernel. Y así, las distancias se miden en el espacio de características de alta dimensionalidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "X, y = make_moons(n_samples=100, random_state=123)\n",
    "XKPCA = KernelPCA(n_components=2, kernel='rbf', gamma=15).fit_transform(X)\n",
    "\n",
    "plt.figure(figsize=(16,6))\n",
    "\n",
    "# figure 1\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(X[y==0, 0], X[y==0, 1], color='red', alpha=0.5)\n",
    "plt.scatter(X[y==1, 0], X[y==1, 1], color='blue', alpha=0.5)\n",
    "\n",
    "plt.xlabel(\"$x_1$\", fontsize=16)\n",
    "plt.ylabel(\"$x_2$\", fontsize=16)\n",
    "\n",
    "# figure 2\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(XKPCA[y==0, 0], XKPCA[y==0, 1], color='red', alpha=0.5)\n",
    "plt.scatter(XKPCA[y==1, 0], XKPCA[y==1, 1], color='blue', alpha=0.5)\n",
    "\n",
    "plt.xlabel(\"$x_1$\", fontsize=16)\n",
    "plt.ylabel(\"$x_2$\", fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\">\n",
    "EJERCICIO 7.6: Aplica Kernel PCA sobre el siguiente ejemplo y representa el resultado.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "\n",
    "X, y = make_circles(n_samples=1000, random_state=123, noise=0.1, factor=0.2)\n",
    "\n",
    "# ... código aquí\n",
    "X_kpca = ...\n",
    "\n",
    "plt.figure(figsize=(16,6))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(X[y==0, 0], X[y==0, 1], color='red', alpha=0.5)\n",
    "plt.scatter(X[y==1, 0], X[y==1, 1], color='blue', alpha=0.5)\n",
    "plt.xlabel(\"$x_1$\", fontsize=16)\n",
    "plt.ylabel(\"$x_2$\", fontsize=16)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(X_kpca[y==0, 0], X_kpca[y==0, 1], color='red', alpha=0.5)\n",
    "plt.scatter(X_kpca[y==1, 0], X_kpca[y==1, 1], color='blue', alpha=0.5)\n",
    "\n",
    "plt.xlabel(\"$x_1$\", fontsize=16)\n",
    "plt.ylabel(\"$x_2$\", fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
