{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f1fea31-8c59-4d52-b30e-ffc17ff0bf0f",
   "metadata": {},
   "source": [
    "# NOTAS PARA LA PRACTICA\n",
    "\n",
    "aaaaaa\n",
    "\n",
    "#### Modelos usados en clase\n",
    "\t1. K-NN --> KNeighborsClassifier\n",
    "\t2. LogisticRegression\n",
    "\n",
    "#### Pasos para limpieza de datos\n",
    "\n",
    "1. Dividir datos de train y test. (Olvidare del DF de test) (Esto siempre se hace)\n",
    "2. Analizar datos, entender bien los datos y Normalizar datos.\n",
    "\t- Dibujar plots\n",
    "\t- Analizar outliers\n",
    "\t- Limpiar da\n",
    "    - tos (remplazar valores null, eliminar outliers, etc)\n",
    "\t- Eliminar campos porque no interesa tenerlo (debe estar justificado el por que) \n",
    "\t- Agregar nuevos campos con base a calculos de los campos originales\n",
    "\t- Etc......\n",
    "\n",
    "\t**NOTAS:**\n",
    "\t\t- Recuerda crear funciones para todo lo que hagas con los datos de train porque luego tienes que hacer lo mismo con los datos de test.\n",
    "\t\t- NO se puede cambiar la semilla que se uso al momento de hacer el split si ya se ha hecho el analisis. NO TIENE SENTIDO UTILIZAR LA SEMILLA COMO UN PARAMETRO DE ENTRENAMIENTO. \n",
    "\t\n",
    "3. Una vez tengas una accurrance razonable si puedes pasar al siguiente paso, si no seguir analizando que puedes ajustar.\n",
    "\n",
    "\n",
    "#### Preguntas\n",
    "- ¿Como saber si no sobreajuste el modulo?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1151bb-3895-419f-9e8b-e9ae237304d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3dba03b-3f72-4499-817a-d6dc73e5d150",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07804dc7-450b-451e-941b-8b0d275498d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2487db81-c233-454a-9c31-abd5714b2b37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "103b6615-ed9a-4133-aadd-ac26e9ca2366",
   "metadata": {},
   "source": [
    "# Preguntas a CHATGPT\n",
    "\n",
    "## ¿Cuál es la funcion en sklearn que te dice cuales features son mas relevantes para un modelo a partir de un dataset?\n",
    "\n",
    "En **scikit-learn** (sklearn), puedes utilizar varias técnicas para evaluar la relevancia de las características (features) de un conjunto de datos. Algunas de las funciones y métodos más comunes para esto son:\n",
    "\n",
    "1. **`SelectFromModel`**: Este es un selector de características basado en un modelo que ya ha sido entrenado. Puedes usarlo con cualquier modelo que tenga un atributo `coef_` o `feature_importances_` (por ejemplo, árboles de decisión, regresión logística, etc.). El modelo selecciona las características más importantes basadas en estos valores.\n",
    "\n",
    "   Ejemplo de uso con un modelo de regresión logística:\n",
    "   ```python\n",
    "   from sklearn.feature_selection import SelectFromModel\n",
    "   from sklearn.linear_model import LogisticRegression\n",
    "   \n",
    "   # Suponiendo que tienes X_train y y_train\n",
    "   model = LogisticRegression()\n",
    "   model.fit(X_train, y_train)\n",
    "   \n",
    "   selector = SelectFromModel(model, threshold=\"mean\")  # Selecciona las características más relevantes\n",
    "   X_selected = selector.transform(X_train)\n",
    "   ```\n",
    "\n",
    "2. **`mutual_info_classif`** y **`mutual_info_regression`**: Estas funciones calculan la información mutua entre las características y el objetivo (target), que puede ser útil para determinar la relevancia de las características, especialmente en problemas no lineales.\n",
    "\n",
    "   Ejemplo de uso para clasificación:\n",
    "   ```python\n",
    "   from sklearn.feature_selection import mutual_info_classif\n",
    "   \n",
    "   # Suponiendo que tienes X_train y y_train\n",
    "   mi_scores = mutual_info_classif(X_train, y_train)\n",
    "   print(mi_scores)\n",
    "   ```\n",
    "\n",
    "3. **`RandomForestClassifier` o `RandomForestRegressor`**: Los modelos de bosques aleatorios (Random Forest) tienen un atributo `feature_importances_` que indica la importancia de cada característica. Este método es muy popular para obtener una clasificación de las características más relevantes.\n",
    "\n",
    "   Ejemplo de uso con RandomForest:\n",
    "   ```python\n",
    "   from sklearn.ensemble import RandomForestClassifier\n",
    "   \n",
    "   # Suponiendo que tienes X_train y y_train\n",
    "   rf = RandomForestClassifier()\n",
    "   rf.fit(X_train, y_train)\n",
    "   \n",
    "   # Importancia de las características\n",
    "   print(rf.feature_importances_)\n",
    "   ```\n",
    "\n",
    "4. **`LinearRegression` o `LogisticRegression`**: Estos modelos también proporcionan un atributo `coef_` que indica la importancia de las características, especialmente en modelos lineales.\n",
    "\n",
    "   Ejemplo de uso con regresión logística:\n",
    "   ```python\n",
    "   from sklearn.linear_model import LogisticRegression\n",
    "   \n",
    "   # Suponiendo que tienes X_train y y_train\n",
    "   model = LogisticRegression()\n",
    "   model.fit(X_train, y_train)\n",
    "   \n",
    "   # Coeficientes del modelo (importancia de las características)\n",
    "   print(model.coef_)\n",
    "   ```\n",
    "\n",
    "#### Resumen\n",
    "Las principales funciones para conocer la relevancia de las características en scikit-learn son:\n",
    "\n",
    "- **`SelectFromModel`**: Selección de características basada en un modelo ya entrenado.\n",
    "- **`mutual_info_classif` / `mutual_info_regression`**: Información mutua para clasificar o regresión.\n",
    "- **`RandomForestClassifier/Regressor`**: Importancia de características basada en árboles aleatorios.\n",
    "- **`LogisticRegression` o `LinearRegression`**: Coeficientes de un modelo lineal.\n",
    "\n",
    "Cada una de estas funciones tiene sus ventajas dependiendo del tipo de modelo y del problema que estés resolviendo.\n",
    "\n",
    "------\n",
    "\n",
    "## ¿Y el de recorter_feature_elimination?\n",
    "\n",
    "¡Claro! **`RFE` (Recursive Feature Elimination)** es otra técnica muy popular en **scikit-learn** para seleccionar las características más relevantes. \n",
    "\n",
    "**RFE** funciona de la siguiente manera:\n",
    "1. Entrena un modelo con todas las características.\n",
    "2. Elimina la característica menos importante (la que tenga el coeficiente más bajo o la menor importancia, según el tipo de modelo).\n",
    "3. Vuelve a entrenar el modelo con las características restantes y repite este proceso hasta que solo quede el número deseado de características.\n",
    "\n",
    "La función **`RFE`** se encuentra en el módulo `sklearn.feature_selection`, y puedes usarla con cualquier modelo que permita evaluar la importancia de las características (por ejemplo, modelos lineales o árboles de decisión).\n",
    "\n",
    "#### Ejemplo de uso de `RFE`:\n",
    "\n",
    "Supongamos que tienes un conjunto de datos de entrenamiento `X_train` y las etiquetas `y_train`, y deseas seleccionar las 5 características más importantes utilizando un modelo de regresión logística:\n",
    "\n",
    "```python\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Suponiendo que tienes X_train y y_train\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Aplicar RFE para seleccionar las 5 características más importantes\n",
    "selector = RFE(model, n_features_to_select=5)\n",
    "selector = selector.fit(X_train, y_train)\n",
    "\n",
    "# Obtener las características seleccionadas\n",
    "print(\"Características seleccionadas:\", selector.support_)  # True si la característica es seleccionada\n",
    "print(\"Ranking de características:\", selector.ranking_)  # Menor es más importante\n",
    "```\n",
    "\n",
    "#### Explicación:\n",
    "- **`model`**: El modelo que se usará para evaluar la importancia de las características.\n",
    "- **`n_features_to_select`**: Especifica cuántas características deseas seleccionar. En este caso, seleccionamos 5 características.\n",
    "- **`selector.support_`**: Devuelve un array de booleanos que indica si cada característica fue seleccionada (True) o no (False).\n",
    "- **`selector.ranking_`**: Devuelve el ranking de las características, donde el número 1 indica la característica más importante.\n",
    "\n",
    "#### ¿Cómo funciona `RFE`?\n",
    "- `RFE` evalúa el rendimiento del modelo y elimina recursivamente las características menos relevantes, basándose en el rendimiento del modelo (por ejemplo, el coeficiente en un modelo lineal o la importancia en un árbol).\n",
    "- El proceso continúa hasta que se alcanza el número deseado de características seleccionadas.\n",
    "\n",
    "#### Ventajas:\n",
    "- **RFE** es muy útil cuando se desea obtener un número reducido de características sin perder rendimiento.\n",
    "- Es aplicable a cualquier modelo de **scikit-learn** que permita la evaluación de la importancia de las características.\n",
    "\n",
    "#### Conclusión:\n",
    "`RFE` es una excelente opción cuando se desea realizar una selección de características de manera sistemática, eliminando las menos relevantes a medida que el modelo avanza.\n",
    "\n",
    "------\n",
    "\n",
    "## ¿A parte de esas que otras opciones hay y cual recomiendas?\n",
    "\n",
    "En **scikit-learn** hay varias técnicas adicionales para la selección de características, además de **RFE** y las que mencioné anteriormente. Cada método tiene sus ventajas según el tipo de datos, el modelo y los objetivos del análisis. Aquí te hago un resumen de otras opciones importantes para la selección de características, y te doy recomendaciones sobre cuál utilizar dependiendo del contexto.\n",
    "\n",
    "### 1. **`SelectKBest`**\n",
    "`SelectKBest` es un selector de características que utiliza una prueba estadística (por ejemplo, *chi-cuadrado*, correlación de Pearson, etc.) para evaluar la relevancia de cada característica.\n",
    "\n",
    "#### Ejemplo con el test *chi-cuadrado*:\n",
    "```python\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Cargar un dataset de ejemplo\n",
    "digits = load_digits()\n",
    "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.3)\n",
    "\n",
    "# Selección de las 10 características más importantes usando chi-cuadrado\n",
    "selector = SelectKBest(chi2, k=10)\n",
    "X_train_selected = selector.fit_transform(X_train, y_train)\n",
    "```\n",
    "\n",
    "**Ventajas**:\n",
    "- Es simple de usar y rápido.\n",
    "- Funciona bien con datos de tipo categórico o cuando hay una relación lineal clara entre las características y el objetivo.\n",
    "  \n",
    "**Limitación**:\n",
    "- Depende de la prueba estadística que se elija, por lo que podría no funcionar bien si las relaciones no son lineales.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **`SelectPercentile`**\n",
    "`SelectPercentile` selecciona las características más relevantes basadas en un porcentaje del total de características. Al igual que `SelectKBest`, se puede usar con diferentes tests estadísticos.\n",
    "\n",
    "#### Ejemplo:\n",
    "```python\n",
    "from sklearn.feature_selection import SelectPercentile, chi2\n",
    "\n",
    "# Selección del 20% de las mejores características\n",
    "selector = SelectPercentile(chi2, percentile=20)\n",
    "X_train_selected = selector.fit_transform(X_train, y_train)\n",
    "```\n",
    "\n",
    "**Ventajas**:\n",
    "- Permite seleccionar un porcentaje específico de características, lo que puede ser útil cuando se tienen muchos atributos.\n",
    "\n",
    "**Limitación**:\n",
    "- Depende de la misma prueba estadística que `SelectKBest`, por lo que también podría no ser efectiva en relaciones no lineales.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **`VarianceThreshold`**\n",
    "`VarianceThreshold` es una técnica de selección de características que elimina las características cuya varianza es menor que un umbral. Es útil cuando se quiere eliminar características constantes o casi constantes.\n",
    "\n",
    "#### Ejemplo:\n",
    "```python\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# Eliminar características cuya varianza es menor a 0.01\n",
    "selector = VarianceThreshold(threshold=0.01)\n",
    "X_train_selected = selector.fit_transform(X_train)\n",
    "```\n",
    "\n",
    "**Ventajas**:\n",
    "- Es muy simple y útil cuando se sabe que algunas características tienen muy poca variabilidad y no aportan información.\n",
    "  \n",
    "**Limitación**:\n",
    "- No toma en cuenta la relación de las características con la variable objetivo, solo se basa en la varianza.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **`f_classif` y `f_regression`**\n",
    "Estos son tests de análisis de varianza (ANOVA) que se utilizan para seleccionar características relevantes para tareas de clasificación y regresión, respectivamente.\n",
    "\n",
    "#### Ejemplo de `f_classif` (para clasificación):\n",
    "```python\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Seleccionar las 5 mejores características usando ANOVA F-value\n",
    "selector = SelectKBest(f_classif, k=5)\n",
    "X_train_selected = selector.fit_transform(X_train, y_train)\n",
    "```\n",
    "\n",
    "**Ventajas**:\n",
    "- Muy útil para problemas de clasificación, cuando se busca la relación entre las características y la variable objetivo.\n",
    "- También funciona bien para problemas de regresión con `f_regression`.\n",
    "\n",
    "**Limitación**:\n",
    "- Asume que las características y el objetivo siguen una distribución normal.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Métodos basados en modelos**:\n",
    "Además de los modelos como `RandomForest` o `LogisticRegression`, hay otras técnicas que se pueden usar con diferentes tipos de modelos.\n",
    "\n",
    "#### **L1 Regularization (Lasso)**\n",
    "El **Lasso** (regresión L1) puede ayudar a seleccionar características al aplicar penalizaciones en los coeficientes del modelo. Las características cuyos coeficientes sean igual a cero pueden considerarse no relevantes.\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# Usar Lasso para seleccionar características\n",
    "lasso = Lasso(alpha=0.01)\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "# Selección de características usando el modelo Lasso\n",
    "selector = SelectFromModel(lasso, threshold=\"mean\")\n",
    "X_train_selected = selector.transform(X_train)\n",
    "```\n",
    "\n",
    "**Ventajas**:\n",
    "- Muy útil para problemas de regresión.\n",
    "- Realiza la selección de características de forma automática, eliminando aquellas que tienen una contribución mínima.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. **Tree-based Methods (e.g., `DecisionTreeClassifier`, `GradientBoosting`)**\n",
    "Los modelos basados en árboles, como los árboles de decisión, **XGBoost**, o **Gradient Boosting**, proporcionan una estimación directa de la importancia de cada característica mediante el atributo `feature_importances_`.\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Ver la importancia de las características\n",
    "print(rf.feature_importances_)\n",
    "```\n",
    "\n",
    "**Ventajas**:\n",
    "- Son muy efectivos en problemas complejos y no lineales.\n",
    "- Proporcionan una evaluación directa de la importancia de las características.\n",
    "\n",
    "**Limitación**:\n",
    "- Los modelos pueden ser más complejos y requieren más tiempo de entrenamiento, especialmente en grandes conjuntos de datos.\n",
    "\n",
    "---\n",
    "\n",
    "#### Recomendaciones:\n",
    "\n",
    "La elección de la técnica depende del tipo de datos y el problema que estés abordando:\n",
    "\n",
    "- **Si tienes muchas características y deseas reducir el conjunto de manera sistemática**, **RFE** o **`SelectKBest`** con una prueba estadística (como ANOVA o *chi-cuadrado*) pueden ser útiles.\n",
    "- **Si sospechas que hay características muy constantes o poco informativas**, prueba **`VarianceThreshold`**.\n",
    "- **Si estás trabajando con un modelo complejo**, como árboles de decisión o modelos de ensamble (e.g., Random Forest, XGBoost), los **atributos de importancia de características** proporcionados por el modelo pueden ser muy efectivos.\n",
    "- **Si estás resolviendo un problema de regresión**, el uso de **Lasso (L1 Regularization)** puede ayudar a reducir el número de características seleccionadas.\n",
    "\n",
    "**Recomendación general**: Si no sabes a priori qué método es el mejor, una buena estrategia inicial es utilizar **RandomForest** o **XGBoost** para obtener una visión general de la importancia de las características y luego usar **RFE** o **Lasso** para ajustar y refinar la selección.\n",
    "\n",
    "¡Todo depende del contexto y los objetivos del análisis!\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03343f30-8882-43a4-86c3-21b5a64590e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
