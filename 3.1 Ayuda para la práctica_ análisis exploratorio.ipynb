{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estudio completo de un problema de ML con Python\n",
    "### Estimación del precio de una vivienda \n",
    "\n",
    "En este notebook haremos un análisis exploratorio básico de la base de datos de viviendas [House Sales in King County, USA](https://www.kaggle.com/harlfoxem/housesalesprediction), para familiarizarnos con los datos y posteriormente aplicar técnicas de machine learning sobre ellos. \n",
    "\n",
    "Lo primero es echar un ojo a la fuente; en este caso es Kaggle, por lo que disponéis de muchísima información sobre los datos (no os acostumbréis).\n",
    "\n",
    "Sabemos que para cada vivienda, se tienen los siguientes atributos, características o features:\n",
    "\n",
    "| Atributo | descripción |\n",
    "| :- |:- |\n",
    "|*id*| identificador de la vivienda|\n",
    "| *date*| fecha\n",
    "| *price*| precio\n",
    "| *bedrooms*| número de habitaciones\n",
    "| *bathrooms*| número de baños/aseos\n",
    "| *sqtf_living*| superficie habitable (en pies al cuadrado)\n",
    "| *sqft_lot*| superficie de la parcela (en pies al cuadrado)\n",
    "| *floors*| número de plantas\n",
    "| *waterfront*| indica si la vivienda tiene acceso a un lago\n",
    "| *view*| tipo de vista (variable numérica)\n",
    "| *condition*| condición de la vivienda (variable númerica)\n",
    "| *grade*| medida de la calidad de la construcción (variable numérica)\n",
    "| *sqft_above*| superficie por encima del suelo (en pies al cuadrado)\n",
    "| *sqft_basement*| superficie del sótano (en pies al cuadrado)\n",
    "| *yr_built*| año de construcción de la vivienda\n",
    "| *yr_renovated*| año de renovación de la vivienda\n",
    "| *lat*| latitud de la parcela\n",
    "| *long*| longitud de la parcela\n",
    "| *sqft_living15*| superficie habitable promedio de los 15 vecinos más cercanos \t\t\t\t\n",
    "| *sqft_lot15*| superficie de la parcela promedio de los 15 vecinos más cercanos\n",
    "\n",
    "Vamos a utilizar **DataFrames** de [Pandas](http://pandas.pydata.org/). Como es sabido, Pandas es un módulo de python de código abierto para el análisis de datos, que proporciona estructuras de datos fáciles de utilizar. Como guía de referencia básica, puede consultarse la [cheat sheet](https://github.com/pandas-dev/pandas/blob/master/doc/cheatsheet/Pandas_Cheat_Sheet.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy  as np  \n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Carga de datos y división train/test\n",
    "\n",
    "Hay que tener mucho cuidado a la hora de realizar la división, para no cometer data leakage. Yo recomiendo que echéis un ojo al dataset, eliminéis todas aquellas columnas que sabéis que podéis quitar gracias a vuestro conocimiento del dominio (ids, URLs, etc) y a continuación dividáis en train/test para evitar riesgos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_data = pd.read_csv(\"./data/king_county.csv\") # cargamos fichero\n",
    "print(house_data.shape)\n",
    "house_data.head(5).T                                 # visualizamos 5 primeras filas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método `train_test_split` es mucho más potente que lo que hemos visto hasta ahora en clase, y permite hacer muchas más cosas. De momento siempre hemos particionado en cuatro (xtrain, xtest, ytrain, ytest) obteniendo arrays de numpy, pero no es la única opción. Aquí tenéis un único fichero .csv, y podéis usar la función para obtener dos subconjuntos: train y test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "full_df = pd.read_csv(\"./data/king_county.csv\")\n",
    "train, test = train_test_split(full_df, test_size=0.2, shuffle=True, random_state=0)\n",
    "\n",
    "print(f'Dimensiones del dataset de training: {train.shape}')\n",
    "print(f'Dimensiones del dataset de test: {test.shape}')\n",
    "\n",
    "# Guardamos\n",
    "train.to_csv('./data/king_county_train.csv', sep=';', decimal='.', index=False)\n",
    "test.to_csv('./data/king_county_test.csv', sep=';', decimal='.', index=False)\n",
    "\n",
    "# A partir de este momento cargamos el dataset de train y trabajamos ÚNICAMENTE con él. \n",
    "\n",
    "house_data = pd.read_csv('./data/king_county_train.csv', sep=';', decimal='.')\n",
    "house_data.head(5).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Análisis exploratorio\n",
    "\n",
    "Podemos analizar la estructura básica del dataset con las funciones de Pandas que ya conocemos: `describe`, `dtypes`, `shape`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\">\n",
    "EJERCICIO 3.6: Aunque no es fácil de ver, hay un outlier *extremo* ahí arriba. ¿Cuál es?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_data.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputación\n",
    "\n",
    "Vemos que la variable `bedrooms` tiene valores ausentes; hay que imputar. Es muy sencillo con pandas, usando fillna:\n",
    "\n",
    "`df[\"Feature\"].fillna(df[\"Feature\"].mode()[0], inplace=True)`\n",
    "\n",
    "Se puede rellenar con la moda (valor más frecuente), en otras ocasiones es preferible usar la media, y en algunos (pocos) casos se puede hacer con ceros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_data['bedrooms'].fillna(house_data['bedrooms'].mode()[0], inplace=True)\n",
    "house_data.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ver qué es esa variable `date`, no encaja demasiado con el resto del dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podemos ordenar un dataframe con esta sintaxis:\n",
    "sorted_df = house_data.sort_values(by='date')\n",
    "sorted_df['date'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y para orden inverso:\n",
    "sorted_df = house_data.sort_values(by='date', ascending=False)\n",
    "sorted_df['date'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que la columna va de 2014 a 2015, y probablemente haga referencia a la fecha de venta. No interesa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Codificación de variables categóricas\n",
    "\n",
    "Podemos observar que todas las variables son de tipo numérico, así que no tenemos que codificar ninguna de ellas. Pero... y si necesitáramos hacerlo? \n",
    "\n",
    "[MeanEncoder](https://maxhalford.github.io/blog/target-encoding/): en sklearn se llama [TargetEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.TargetEncoder.html). Asigna un valor a cada variable categórica según la media de la columna objetivo para el conjunto de registros que tienen esa variable categórica. Es decir, si quisiera categorizar la variable \"Barrio\" con un ME, lo que tendría que hacer es calcular la media de precio en cada barrio (Villaverde, Chamberí, etc) y sustituir el nombre del barrio por esa media. Ojito con el data leakage, os dejo un ejemplo debajo de cómo hacerlo bien.\n",
    "\n",
    "Tutoriales sobre codificación de variables categóricas: [Tutorial 1](https://towardsdatascience.com/encoding-categorical-features-21a2651a065c), [tutorial 2](https://towardsdatascience.com/all-about-categorical-variable-encoding-305f3361fd02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target encoder hace algo así por debajo:\n",
    "\n",
    "\"\"\"\n",
    "categorical = ['cat1', 'cat2', 'cat3']\n",
    "\n",
    "mean_map = {}\n",
    "for c in categorical:\n",
    "    mean = data.groupby(c)['target'].mean()\n",
    "    data[c] = data[c].map(mean)    \n",
    "    mean_map[c] = mean\n",
    "\n",
    "# Si hubiera test, luego se haría:\n",
    "#for c in categorical:\n",
    "#    data_test[c] = data_test[c].map(mean_map[c])\n",
    "\n",
    "data.head()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este es uno de los tutoriales más completos que he visto: [Encoding done the right way](https://maxhalford.github.io/blog/target-encoding/) y me hace especial gracia porque dice esto: \"Label encoding is useless and you should never use it\". No estoy 100% de acuerdo obviamente :) pero sí que es cierto que cuando hay muchas categorías (es decir, no es binario) un LE puede llevar a errores porque asigna números a cada una de ellas, con lo cual el algoritmo puede \"aprender\" erróneamente. Supongamos que tengo una categoría barrio que quiero categorizar:\n",
    "\n",
    "- Barrio céntrico moderno y caro -> LE -> 1\n",
    "- Otro barrio -> LE -> 2\n",
    "- Otro más -> LE -> 3\n",
    "- Y otro -> LE -> 4\n",
    "- Barrio periférico y obrero -> LE -> 5\n",
    "\n",
    "Mis categorías tras el LE pasarían a ser 1-5, pero qué quiere decir esto? Que 5 es mayor que 1? Que 3 es menor que 5? No, porque no existe esa relación entre los barrios, pero SÍ entre los números! Entonces el algoritmo podría decidir que de alguna manera \"Barrio periférico y obrero > Barrio céntrico moderno y caro\" porque 5 > 1.\n",
    "\n",
    "Otro encoder que podéis usar y que va bastante bien es el [OrdinalEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Volviendo al análisis, los atributos *id* y *date* no nos aportan información, así que los descartamos del DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminamos las columnas id y date \n",
    "house_data = house_data.drop(['id','date'], axis=1)\n",
    "house_data.head(5).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes seguir con nuestro análisis, vamos a transformar las variables de superficie para expresarlas en $m^2$. Posteriormente, renombraremos las columnas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pies = [item for item in list(house_data.columns) if 'sqft' in item]\n",
    "pies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convertir las variables en pies al cuadrado en metros al cuadrado \n",
    "feetFeatures = ['sqft_living','sqft_lot','sqft_above','sqft_basement','sqft_living15','sqft_lot15']\n",
    "\n",
    "def sqft_to_m2(superficie):\n",
    "    return superficie * 0.3048 * 0.3048\n",
    "\n",
    "house_data[feetFeatures] = house_data[feetFeatures].apply(sqft_to_m2)\n",
    "\n",
    "# Alternativa usando una función lambda\n",
    "# house_data[feetFeatures] = house_data[feetFeatures].apply(lambda x: x * 0.3048 * 0.3048)\n",
    "\n",
    "# renombramos\n",
    "house_data.columns = ['price','bedrooms','bathrooms','sqm_living','sqm_lot','floors','waterfront','view','condition',\n",
    "                      'grade','sqm_above','sqm_basement','yr_built','yr_renovated','zip_code','lat','long',\n",
    "                      'sqm_living15','sqm_lot15']\n",
    "\n",
    "# visualizamos\n",
    "house_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualización (y más análisis)\n",
    "\n",
    "Una buena práctica es intentar resumir toda la información posible de los datos. Habitualmente nos interesa saber la media y desviación estándar, posiblemente cuartiles de cada una de las variables. Esto nos permitirá, por una lado, tener una idea de cómo son las ditribuciones de cada una de las variables y por otra, nos permitirá verificar si existen datos anómalos, también conocidos como [**outliers**](https://en.wikipedia.org/wiki/Outlier). \n",
    "\n",
    "Además, conviene siempre hacer representaciones gráficas, que nos ofrecen, en general un mejor entendimiento de los datos. Para ello vamos representar los histogramas de algunos atributos: *bedrooms*, *sqm_living* y *yr_built*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "house_data['bedrooms'].plot.hist(alpha=0.5, bins=25, grid = True)\n",
    "#plt.axis([0, 10, 0, 10000])\n",
    "plt.ylim(0, 15)\n",
    "plt.xlabel('bedrooms')\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "house_data['sqm_living'].plot.hist(alpha=0.5, bins=25, grid = True)\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel('sqm_living')\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "house_data['yr_built'].plot.hist(alpha=0.5, bins=25, grid = True)\n",
    "plt.xlabel('yr_built')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez que hemos analizado las variables por separado, el siguiente paso en un análisis exploratorio sería el entender las relaciones entre cada una de las variables/atributos ($\\mathbf{x}$) y la variable respuesta ($y$). \n",
    "\n",
    "Para ello vamos a utilizar un [scatter plot](https://en.wikipedia.org/wiki/Scatter_plot) con la variable objetivo definida $y$ como variable dependiente, y alguna una de las variables explicativas como variables independientes. En el caso de la variable *waterfront*, dado que ésta es binaria, vamos a utilizar un [boxplot](https://en.wikipedia.org/wiki/Box_plot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sólo representamos 3: bedrooms, sqm_living y waterfront\n",
    "# el resto se puede repetir una a una\n",
    "\n",
    "house_data.plot(kind = 'scatter',x='bedrooms',y = 'price')\n",
    "plt.xlabel('# bedrooms')\n",
    "plt.ylabel('price ($)')\n",
    "plt.show()\n",
    "\n",
    "house_data.plot(kind = 'scatter',x='sqm_living',y = 'price')\n",
    "plt.xlabel('sqm_living ($m^2$)')\n",
    "plt.ylabel('price ($)')\n",
    "plt.show()\n",
    "\n",
    "house_data.boxplot(by='waterfront',column = 'price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos estudiar algunas de las variables con `.value_counts()`, por ejemplo *waterfront* o *bedrooms*.\n",
    "\n",
    "<div class = \"alert alert-success\">\n",
    "EJERCICIO 3.7: Analizar los `value_counts` de *waterfront* y *bedrooms*\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eliminación de outliers\n",
    "\n",
    "Tanto con los scatter plot de arriba como con los análisis de `value_counts`, vemos que hay unos pocos outliers en la variable bedrooms (y en sqm_living). Vamos a eliminarlos con un filtro. Si recordáis, vimos en el repaso de Pandas que los dataframes podían filtrarse con `df_filtered = df[condición]`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_data_no_outliers_bedrooms = house_data[house_data['bedrooms'] <= 8]\n",
    "\n",
    "house_data_no_outliers_bedrooms.plot(kind = 'scatter',x='bedrooms',y = 'price')\n",
    "plt.xlabel('# bedrooms')\n",
    "plt.ylabel('price ($)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Hemos perdido muchos datos con la eliminación?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f'Original: {house_data.shape[0]} // '\n",
    "    f'Modificado: {house_data_no_outliers_bedrooms.shape[0]}\\nDiferencia: {house_data.shape[0] - house_data_no_outliers_bedrooms.shape[0]}'\n",
    ")\n",
    "print(f'Variación: {((house_data.shape[0] - house_data_no_outliers_bedrooms.shape[0])/house_data.shape[0])*100:2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\">\n",
    "EJERCICIO 3.8: Repetir el estudio de outliers para la variable *sqm_living*\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez que hemos hecho un primer análisis exploratorio, el siguiente paso consiste en evaluar las correlaciones entre las diferente variables del problema. Habitualmente, esto nos puede servir para identificar posibles atributos que estén altamente correlacionados. \n",
    "\n",
    "Si la correlación entre dos atributos es muy grande, se dice que la matriz de atributos es singular, y como ya vimos, esto es una fuente de error importante en algunos algoritmos de machine learning, como por ejemplo en el caso de la [regresión lineal](https://es.wikipedia.org/wiki/Regresión_lineal). \n",
    "\n",
    "Este problema se denomina *colinealidad*. Para hacer frente a él, normalmente se evalúa [coeficiente de correlación](https://es.wikipedia.org/wiki/Coeficiente_de_correlación_de_Pearson) ($\\rho$) entre las diferentes atributos de tal forma que se descartan que tengan un $\\rho$ superior a un umbral que establezcamos a priori ($|\\rho|>0.9$, por ejemplo). Hay que tener en cuenta que $-1<\\rho<1$, de tal forma que valores próximos a $0$ indican que no hay correlación y valores próximos a $1$ o $-1$ indican una alta correlación.\n",
    "\n",
    "La matriz de correlación se puede sacar de Pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_data.corr() # matriz de correlación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pero es mejor representarla:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr = np.abs(house_data.drop(['price'], axis=1).corr())\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.zeros_like(corr, dtype=bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, mask=mask,vmin = 0.0, vmax=1.0, center=0.5,\n",
    "            linewidths=.1, cmap=\"YlGnBu\", cbar_kws={\"shrink\": .8})\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basándonos en el estudio podríamos llegar a valorar eliminar la variable *sqm_above*.\n",
    "\n",
    "Por último, podemos hacer una representación (scatter_plot) de todas las variables frente al resto, para tener una idea de cómo se relacionan las variables del problema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(house_data, alpha=0.2, figsize=(20, 20), diagonal = 'kde')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generación de nuevas características\n",
    "\n",
    "Este sería el momento de pensar sobre otras variables que tuvieran sentido. Por ejemplo:\n",
    "\n",
    "- Construir el atributo antigüedad de la casa en vez de año de la construcción.\n",
    "- Intentar capturar la relación entre dormitorios y baños\n",
    "- Cualquier otra idea que tengáis. Por ejemplo, para este dataset en concreto la gente encontró que elevar al cuadrado el número de dormitorios era una variable relevante; la explicación (o eso se cree) es que muchas de estas casas habían subdividido habitaciones para poder introducir a más inquilinos, a veces incluso a costa de las zonas comunes. Por tanto, un número de dormitorios mayor era mejor _hasta cierto punto_, lo cual quedaba recogido al elevar al cuadrado. Fijaos como en el scatter plot de bedroom podemos ver esta misma tendencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_data['years']            = 2015 - house_data['yr_built']\n",
    "house_data['bedrooms_squared'] = house_data['bedrooms'].apply(lambda x: x**2)\n",
    "house_data['bed_bath_rooms']   = house_data['bedrooms']*house_data['bathrooms']\n",
    "\n",
    "# ... etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Modelado, cross-validation y estudio de resultados en train y test\n",
    "\n",
    "Ha llegado el gran momento! Antes de modelar, tenemos que cargar los datos de test y aplicar exactamente las mismas transformaciones. Es buena práctica, llegado este momento, combinar todo nuestro preprocesamiento en una única celda:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga de datos\n",
    "house_data = pd.read_csv('./data/king_county_train.csv', sep=';', decimal='.')\n",
    "\n",
    "# Imputación\n",
    "house_data['bedrooms'].fillna(house_data['bedrooms'].mode()[0], inplace=True)\n",
    "\n",
    "# Eliminamos las columnas id y date \n",
    "house_data = house_data.drop(['id','date'], axis=1)\n",
    "\n",
    "# Convertimos las variables en pies al cuadrado en metros al cuadrado y renombramos\n",
    "feetFeatures = ['sqft_living','sqft_lot','sqft_above','sqft_basement','sqft_living15','sqft_lot15']\n",
    "\n",
    "def sqft_to_m2(superficie):\n",
    "    return superficie * 0.3048 * 0.3048\n",
    "\n",
    "house_data[feetFeatures] = house_data[feetFeatures].apply(sqft_to_m2)\n",
    "house_data.columns = ['price','bedrooms','bathrooms','sqm_living','sqm_lot','floors','waterfront','view','condition',\n",
    "                      'grade','sqm_above','sqm_basement','yr_built','yr_renovated','zip_code','lat','long',\n",
    "                      'sqm_living15','sqm_lot15']\n",
    "\n",
    "# Eliminamos outliers en bedrooms\n",
    "house_data = house_data[house_data['bedrooms'] <= 8]\n",
    "\n",
    "# Generamos características\n",
    "house_data['years']            = 2015 - house_data['yr_built']\n",
    "house_data['bedrooms_squared'] = house_data['bedrooms'].apply(lambda x: x**2)\n",
    "house_data['bed_bath_rooms']   = house_data['bedrooms']*house_data['bathrooms']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y ahora aplicamos fácilmente a test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga de datos\n",
    "house_data_test = pd.read_csv('./data/king_county_test.csv', sep=';', decimal='.')\n",
    "\n",
    "# Imputación\n",
    "house_data_test['bedrooms'].fillna(house_data_test['bedrooms'].mode()[0], inplace=True)\n",
    "\n",
    "# Eliminamos las columnas id y date \n",
    "house_data_test = house_data_test.drop(['id','date'], axis=1)\n",
    "\n",
    "# Convertimos las variables en pies al cuadrado en metros al cuadrado y renombramos\n",
    "feetFeatures = ['sqft_living','sqft_lot','sqft_above','sqft_basement','sqft_living15','sqft_lot15']\n",
    "\n",
    "def sqft_to_m2(superficie):\n",
    "    return superficie * 0.3048 * 0.3048\n",
    "\n",
    "house_data_test[feetFeatures] = house_data_test[feetFeatures].apply(sqft_to_m2)\n",
    "house_data_test.columns = ['price','bedrooms','bathrooms','sqm_living','sqm_lot','floors','waterfront','view','condition',\n",
    "                      'grade','sqm_above','sqm_basement','yr_built','yr_renovated','zip_code','lat','long',\n",
    "                      'sqm_living15','sqm_lot15']\n",
    "\n",
    "# Eliminamos outliers en bedrooms\n",
    "house_data_test = house_data_test[house_data_test['bedrooms'] <= 8]\n",
    "\n",
    "# Generamos características\n",
    "house_data_test['years']            = 2015 - house_data_test['yr_built']\n",
    "house_data_test['bedrooms_squared'] = house_data_test['bedrooms'].apply(lambda x: x**2)\n",
    "house_data_test['bed_bath_rooms']   = house_data_test['bedrooms']*house_data_test['bathrooms']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos preparar los datos para sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "# Dataset de train\n",
    "data_train = house_data.values\n",
    "y_train = data_train[:,0:1]     # nos quedamos con la 1ª columna, price\n",
    "X_train = data_train[:,1:]      # nos quedamos con el resto\n",
    "\n",
    "# Dataset de test\n",
    "data_test = house_data_test.values\n",
    "y_test = data_test[:,0:1]     # nos quedamos con la 1ª columna, price\n",
    "X_test = data_test[:,1:]      # nos quedamos con el resto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y si queremos, podemos normalizar, pero con los datos de train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escalamos (con los datos de train)\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "XtrainScaled = scaler.transform(X_train)\n",
    "\n",
    "# recordad que esta normalización/escalado la realizo con el scaler anterior, basado en los datos de training!\n",
    "XtestScaled = scaler.transform(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Datos entrenamiento: ', XtrainScaled.shape)\n",
    "print('Datos test: ', XtestScaled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vendría lo-de-siempre: cross validation, búsqueda de los parámetros óptimos, visualización de performance vs complejidad..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "alpha_vector = np.logspace(-1,10,20)\n",
    "param_grid = {'alpha': alpha_vector }\n",
    "grid = GridSearchCV(Lasso(), scoring= 'neg_mean_squared_error', param_grid=param_grid, cv = 3, verbose=2)\n",
    "grid.fit(XtrainScaled, y_train)\n",
    "print(\"best mean cross-validation score: {:.3f}\".format(grid.best_score_))\n",
    "print(\"best parameters: {}\".format(grid.best_params_))\n",
    "\n",
    "#-1 porque es negado\n",
    "scores = -1*np.array(grid.cv_results_['mean_test_score'])\n",
    "plt.semilogx(alpha_vector,scores,'-o')\n",
    "plt.xlabel('alpha',fontsize=16)\n",
    "plt.ylabel('3-Fold MSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "alpha_optimo = grid.best_params_['alpha']\n",
    "lasso = Lasso(alpha = alpha_optimo).fit(XtrainScaled,y_train)\n",
    "\n",
    "ytrainLasso = lasso.predict(XtrainScaled)\n",
    "ytestLasso  = lasso.predict(XtestScaled)\n",
    "mseTrainModelLasso = mean_squared_error(y_train,ytrainLasso)\n",
    "mseTestModelLasso = mean_squared_error(y_test,ytestLasso)\n",
    "\n",
    "print('MSE Modelo Lasso (train): %0.3g' % mseTrainModelLasso)\n",
    "print('MSE Modelo Lasso (test) : %0.3g' % mseTestModelLasso)\n",
    "\n",
    "print('RMSE Modelo Lasso (train): %0.3g' % np.sqrt(mseTrainModelLasso))\n",
    "print('RMSE Modelo Lasso (test) : %0.3g' % np.sqrt(mseTestModelLasso))\n",
    "\n",
    "feature_names = house_data.columns[1:] # es igual en train y en test\n",
    "\n",
    "w = lasso.coef_\n",
    "for f,wi in zip(feature_names,w):\n",
    "    print(f,wi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y a partir de aquí, habría que iterar. Los coeficientes son muy grandes; quizá queramos regularizar. O probar otros modelos. O reducir características. O..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
