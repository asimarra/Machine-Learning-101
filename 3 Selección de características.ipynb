{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Métodos de filtrado\n",
    "\n",
    "En este notebook se revisarán los conceptos de:\n",
    "\n",
    "1. Métodos de filtrado para regresión\n",
    "2. Métodos de filtrado para clasificación\n",
    "3. Métodos de filtrado en un problema realista\n",
    "4. Métodos *wrapper*\n",
    "5. Métodos *embedded*\n",
    "\n",
    "Primero cargamos librerías y funciones necesarias, incluyendo las del módulo `utils`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import CM_BRIGHT\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Métodos de filtrado para regresión\n",
    "\n",
    "Vamos a analizar un par de métodos de [filtrado](http://scikit-learn.org/stable/modules/feature_selection.html) para regresión mediante un ejemplo sencillo:\n",
    "\n",
    "* [f_regression](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_regression.html#sklearn.feature_selection.f_regression)\n",
    "* [mutual_info_regression](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_regression.html#sklearn.feature_selection.mutual_info_regression)\n",
    "\n",
    "#### Ejemplo \n",
    "Se tienen tres variables $x_1, x_2,x_3$. Las tres son aleatorias y se distribuyen uniformemente en el intervalo $[0,1]$. La salida depende de estas variables de la forma: \n",
    "\n",
    "$$y = x_1 + \\sin{(6\\pi x_2)} + 0.1\\mathcal{N}(0, 1),$$\n",
    "\n",
    "Esto es:\n",
    "\n",
    "* $y$ depende linealmente de $x_1$\n",
    "* $y$ depende no linealmente de $x_2$\n",
    "* $y$ no depende de $x_3$\n",
    "\n",
    "Por tanto, $x_3$ es una variable irrelevante para $y$. Veamos qué nos dicen nuestros test de filtrado de selección de características."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Este ejemplo, tomado de:\n",
    "# http://scikit-learn.org/stable/auto_examples/feature_selection/plot_f_test_vs_mi.html#sphx-glr-auto-examples-feature-selection-plot-f-test-vs-mi-py\n",
    "\n",
    "from sklearn.feature_selection import f_regression, mutual_info_regression\n",
    "\n",
    "np.random.seed(0)\n",
    "X = np.random.rand(1000, 3)\n",
    "y = X[:, 0] + np.sin(6 * np.pi * X[:, 1]) + 0.1 * np.random.randn(1000)\n",
    "\n",
    "f_test, _ = f_regression(X, y)\n",
    "f_test /= np.max(f_test)\n",
    "\n",
    "mi = mutual_info_regression(X, y)\n",
    "mi /= np.max(mi)\n",
    "\n",
    "# Visualización\n",
    "plt.figure(figsize=(15, 5))\n",
    "for i in range(3):\n",
    "    plt.subplot(1, 3, i + 1)\n",
    "    plt.scatter(X[:, i], y, edgecolor='black', s=20)\n",
    "    plt.xlabel(\"$x_{}$\".format(i + 1), fontsize=14)\n",
    "    if i == 0:\n",
    "        plt.ylabel(\"$y$\", fontsize=14)\n",
    "    plt.title(\"F-test={:.2f}, MI={:.2f}\".format(f_test[i], mi[i]),\n",
    "              fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\">\n",
    "EJERCICIO 3.1: ¿Cómo interpretáis estos resultados?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Métodos de filtrado para clasificación\n",
    "\n",
    "Vamos a analizar los mismos métodos de [filtrado](http://scikit-learn.org/stable/modules/feature_selection.html) que en el caso de regresión, aplicados ahora para distintos problemas de clasificación:\n",
    "\n",
    "* [f_classif](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_classif.html#sklearn.feature_selection.f_classif)\n",
    "* [mutual_info_classif](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_classif.html#sklearn.feature_selection.mutual_info_classif)\n",
    "\n",
    "### 2.1 Problema de clasificación linealmente separable\n",
    "\n",
    "En este problema, la variable $x_1$ define un problema linealmente separable y constituye la variable informativa. $x_2$ representa una variable redundante, mientras que $x_3$ y $x_4$ son variables ruidosas que no aportan información.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toy example: linearly separable problem\n",
    "np.random.seed(0)\n",
    "\n",
    "# Parámetros\n",
    "N      = 1000\n",
    "mu     = 2\n",
    "sigma1 = 1\n",
    "sigma2 = 5\n",
    "\n",
    "# Variables auxiliares\n",
    "unos  = np.ones(int(N/2))\n",
    "rand2 = np.random.randn(int(N/2),1)\n",
    "\n",
    "# Características\n",
    "y  = np.concatenate([-1*unos,unos]) \n",
    "X1 = np.concatenate([-mu + sigma1*rand2,mu + sigma1*rand2])\n",
    "X2 = sigma2*np.random.randn(N,1) - 3*X1\n",
    "\n",
    "X3 = 2*np.random.randn(N,1)\n",
    "X4 = np.random.rand(N,1)\n",
    "\n",
    "X  = np.hstack((X1,X2,X3,X4))\n",
    "\n",
    "# Visualización\n",
    "plt.figure(figsize=(12, 14))\n",
    "for i in range(4):\n",
    "    plt.subplot(3, 2, i + 1)\n",
    "    plt.hist(X[y<0,i],bins=20, density=True, alpha=0.5, label='-1',color='b')\n",
    "    plt.hist(X[y>0,i], bins=20, density=True, alpha=0.5, label='+1',color='r')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.xlabel(\"$x_{}$\".format(i + 1), fontsize=18)\n",
    "\n",
    "plt.subplot(3, 2, 5)\n",
    "plt.scatter(X1,X2,c=y.reshape(-1,1),cmap=CM_BRIGHT)\n",
    "plt.xlabel(\"$x_1$\", fontsize=16)\n",
    "plt.ylabel(\"$x_2$\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apliquemos los métodos de filtrado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import f_classif, mutual_info_classif\n",
    "\n",
    "featureNames = ['x1','x2','x3','x4']\n",
    "\n",
    "# do calculations\n",
    "f_test, _ = f_classif(X, y)\n",
    "f_test /= np.max(f_test)\n",
    "\n",
    "mi = mutual_info_classif(X, y)\n",
    "mi /= np.max(mi)\n",
    "\n",
    "# do some plotting\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.bar(range(X.shape[1]),f_test,  align=\"center\")\n",
    "plt.xticks(range(X.shape[1]),featureNames)\n",
    "plt.xlabel('features')\n",
    "plt.ylabel('Ranking')\n",
    "plt.title('$F-test$ score')\n",
    "\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.bar(range(X.shape[1]),mi,  align=\"center\")\n",
    "plt.xticks(range(X.shape[1]),featureNames)\n",
    "plt.xlabel('features')\n",
    "plt.ylabel('Ranking')\n",
    "plt.title('Mutual information score')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\">\n",
    "EJERCICIO 3.2: ¿Cómo interpretáis estos resultados?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Problema de clasificación no linealmente separable\n",
    "\n",
    "Vamos a analizar el problema XOR que, como ya sabéis, está definido (es decir, resuelto) por las variables $x_1$ y $x_2$; recordad lo de los cuadrantes. Añadimos además dos variables relevantes $x_3$ y $x_4$, con capacidad discriminatoria y que por tanto pueden ayudar a mejorar las prestaciones del problema de clasificación bajo estudio. Por último, se incluyen 20 variables ruidosas irrelevantes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "# -- parameters\n",
    "N     = 1000\n",
    "mu    = 1.5      # Cambia este valor\n",
    "sigma = 1      # Cambia este valor\n",
    "\n",
    "# variables auxiliares\n",
    "unos = np.ones(int(N/4))\n",
    "random4 = sigma*np.random.randn(int(N/4),1)\n",
    "random2 = sigma*np.random.randn(int(N/2),1)\n",
    "\n",
    "# -- features\n",
    "y = np.concatenate([-1*unos,       unos,          unos,         -1*unos]) \n",
    "X1 = np.concatenate([-mu + random4, mu + random4, -mu + random4, mu + random4])\n",
    "X2 = np.concatenate([+mu + random2,               -mu + random2])\n",
    "\n",
    "# fijaos como X3 y X4 son combinaciones de X1 y X2; sabemos que el problema se soluciona con ambas variables,\n",
    "# pero que ninguna de las dos por separado nos sirven. X3 y X4 tienen cierta capacidad discriminatoria\n",
    "X3 = 3*(X1+X2) + np.sqrt(2)*np.random.randn(N,1)\n",
    "X4 = 2*np.square((X1+X2)) + np.sqrt(2)*np.random.randn(N,1)\n",
    "\n",
    "E  = 2*np.random.randn(N, 20) # noisy variables\n",
    "\n",
    "X  = np.hstack((X1,X2,X3,X4,E))\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.subplot(2, 2, 1)\n",
    "\n",
    "plt.scatter(X1,X2,c=y.reshape(-1,1), cmap=CM_BRIGHT)\n",
    "plt.xlabel(\"$x_1$\", fontsize=16)\n",
    "plt.ylabel(\"$x_2$\", fontsize=16)\n",
    "\n",
    "for i in range(3):\n",
    "    plt.subplot(2, 2, i + 2)\n",
    "    plt.hist(X[y<0,i+1],bins=20, density=True, alpha=0.5, label='-1',color='b')\n",
    "    plt.hist(X[y>0,i+1], bins=20, density=True, alpha=0.5, label='+1',color='r')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.xlabel(\"$x_{}$\".format(i + 2), fontsize=18)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# son 24 variables porque hay 4 declaradas arriba (X1, X2, X3, X4) y 20 variables aleatorias que son ruido: en total 24.\n",
    "featureNames = ['x1','x2','x3','x4','x5','x6','x7','x8','x9','x10','x11','x12','x13','x14',\n",
    "                'x15','x16','x17','x18','x19','x20','x21','x22','x23','x24']\n",
    "\n",
    "\n",
    "# do calculations\n",
    "f_test, _ = f_classif(X, y)\n",
    "f_test /= np.max(f_test)\n",
    "\n",
    "mi = mutual_info_classif(X, y)\n",
    "mi /= np.max(mi)\n",
    "\n",
    "# do some plotting\n",
    "plt.figure(figsize=(20, 5))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.bar(range(X.shape[1]),f_test,  align=\"center\")\n",
    "plt.xticks(range(X.shape[1]),featureNames, rotation = 45)\n",
    "plt.xlabel('features')\n",
    "plt.ylabel('Ranking')\n",
    "plt.title('$F Test$ score')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.bar(range(X.shape[1]),mi,  align=\"center\")\n",
    "plt.xticks(range(X.shape[1]),featureNames, rotation = 45)\n",
    "plt.xlabel('features')\n",
    "plt.ylabel('Ranking')\n",
    "plt.title('Mutual information score')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\">\n",
    "EJERCICIO 3.3: ¿Cómo interpretáis estos resultados?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Métodos de filtrado sobre problema realista\n",
    "\n",
    "Vamos a aplicar los métodos de filtrado sobre la base de datos  de viviendas [House Sales in King County, USA](https://www.kaggle.com/harlfoxem/housesalesprediction)\n",
    "\n",
    "Recordamos que para cada vivienda, se tienen los siguientes atributos, características o features:\n",
    "\n",
    "| Atributo | descripción |\n",
    "| :- |:- |\n",
    "|*id*| identificador de la vivienda|\n",
    "| *date*| fecha\n",
    "| *price*| precio\n",
    "| *bedrooms*| número de habitaciones\n",
    "| *bathrooms*| número de baños/aseos\n",
    "| *sqtf_living*| superficie habitable (en pies al cuadrado)\n",
    "| *sqft_lot*| superficie de la parcela (en pies al cuadrado)\n",
    "| *floors*| número de plantas\n",
    "| *waterfront*| indica si la vivienda tiene acceso a un lago\n",
    "| *view*| tipo de vista (variable numérica)\n",
    "| *condition*| condición de la vivienda (variable númerica)\n",
    "| *grade*| medida de la calidad de la construcción (variable numérica)\n",
    "| *sqft_above*| superficie por encima del suelo (en pies al cuadrado)\n",
    "| *sqft_basement*| superficie del sótano (en pies al cuadrado)\n",
    "| *yr_built*| año de construcción de la vivienda\n",
    "| *yr_renovated*| año de renovación de la vivienda\n",
    "| *lat*| latitud de la parcela\n",
    "| *long*| longitud de la parcela\n",
    "| *sqft_living15*| superficie habitable promedio de los 15 vecinos más cercanos \t\t\t\t\n",
    "| *sqft_lot15*| superficie de la parcela promedio de los 15 vecinos más cercanos\n",
    "\n",
    "NOTA: No me detengo mucho en este dataset porque en el siguiente notebook vamos a darle bastante!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veremos este análisis en detalle en el notebook de análisis exploratorio - por ahora, creéroslo!\n",
    "\n",
    "# cargamos datos\n",
    "house_data = pd.read_csv(\"./data/kc_house_data.csv\")\n",
    "\n",
    "# Eliminamos las columnas id y date \n",
    "house_data = house_data.drop(['id','date'], axis=1)\n",
    "\n",
    "# convertimos las variables en pies al cuadrado en metros al cuadrado \n",
    "feetFeatures = ['sqft_living','sqft_lot','sqft_above','sqft_basement','sqft_living15','sqft_lot15']\n",
    "house_data[feetFeatures] = house_data[feetFeatures].apply(lambda x: x * 0.3048 * 0.3048)\n",
    "\n",
    "# renombramos\n",
    "house_data.columns = ['price','bedrooms','bathrooms','sqm_living','sqm_lot','floors','waterfront','view','condition',\n",
    "                      'grade','sqm_above','sqm_basement','yr_built','yr_renovated','zip_code','lat','long',\n",
    "                      'sqm_living15','sqm_lot15']\n",
    "\n",
    "# añadimos las nuevas variables\n",
    "house_data['years']            = 2017 - house_data['yr_built']\n",
    "house_data['bedrooms_squared'] = house_data['bedrooms'].apply(lambda x: x**2)\n",
    "house_data['bed_bath_rooms']   = house_data['bedrooms']*house_data['bathrooms']\n",
    "house_data['log_sqm_living']   = house_data['sqm_living'].apply(lambda x: np.log(x))\n",
    "house_data['lat_plus_long']    = house_data['lat']*house_data['long']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_data.head().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a analizar la correlación entre variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr = np.abs(house_data.drop(['price'], axis=1).corr())\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.zeros_like(corr, dtype=bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, mask=mask,vmin = 0.0, vmax=1.0, center=0.5,\n",
    "            linewidths=.1, cmap=\"YlGnBu\", cbar_kws={\"shrink\": .8})\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convertimos el DataFrame al formato necesario para scikit-learn\n",
    "data = house_data.values \n",
    "\n",
    "y = data[:,0:1]     # nos quedamos con la 1ª columna, price\n",
    "X = data[:,1:]      # nos quedamos con el resto\n",
    "\n",
    "feature_names = house_data.columns[1:]\n",
    "\n",
    "# estudiamos los dos métodos de filtrado\n",
    "f_test, _ = f_regression(X, y)\n",
    "f_test /= np.max(f_test)\n",
    "\n",
    "mi = mutual_info_regression(X, y)\n",
    "mi /= np.max(mi)\n",
    "\n",
    "# visualización\n",
    "plt.figure(figsize=(20, 5))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.bar(range(X.shape[1]),f_test,  align=\"center\")\n",
    "plt.xticks(range(X.shape[1]),feature_names, rotation = 90)\n",
    "plt.xlabel('features')\n",
    "plt.ylabel('Ranking')\n",
    "plt.title('$F Test$ score')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.bar(range(X.shape[1]),mi, align=\"center\")\n",
    "plt.xticks(range(X.shape[1]),feature_names, rotation = 90)\n",
    "plt.xlabel('features')\n",
    "plt.ylabel('Ranking')\n",
    "plt.title('Mutual information score')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\">\n",
    "EJERCICIO 3.4: Las variables seleccionadas, ¿concuerdan con tu intuición?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Métodos Wrapper\n",
    "\n",
    "Estos métodos utilizan un algoritmo de machine learning como caja negra para rankear distintos subconjuntos de variables de acuerdo a su capacidad predictiva. Normalmente se usan en mediante procedimientos hacia delante/detrás en combinación con validación cruzada. Vamos a ver, simplemente, cómo aplicarlos correctamente.\n",
    "\n",
    "#### *The wrong and right way to do cross-validation*\n",
    "\n",
    "Este ejemplo ha sido inspirado en *7.10.2 The Wrong and Right Way to Do Cross-validation* del libro\n",
    "\"The Elements of Statistical Learning\". Hastie, Tibshirani, Friedman\n",
    "\n",
    "Este código ejecuta 500 veces un experimento: en cada iteración, genera un nuevo dataset, donde los predictores son variables aleatorias, ruidosas; a continuación, realiza una selección de características, y luego utiliza un KNN con `n_neighbours=1` para evaluar las prestaciones del subconjunto seleccionado, con validación cruzada de 5-folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "\n",
    "# This is the WRONG way\n",
    "\n",
    "np.random.seed(0)\n",
    "score = []\n",
    "\n",
    "for i in range(500): # This is run for a number of experiments. Montecarlo simulation\n",
    "    \n",
    "    # Create toy example\n",
    "    N = 50\n",
    "    y = np.concatenate([-1*np.ones(int(N/2)),np.ones(int(N/2))]) # target\n",
    "    X = np.random.randn(N, 5000)                       # predictors are random variables!!\n",
    "    \n",
    "    # Note here, the ranking and selection is performed outside the CV loop\n",
    "    f_test, _ = f_classif(X, y)\n",
    "    f_test /= np.max(f_test)\n",
    "    \n",
    "    \n",
    "    ranking = np.argsort(f_test)[::-1] \n",
    "    selected = ranking[0:50]\n",
    "    Xs = X[:,selected]\n",
    "    \n",
    "    # 1-neighbor classifier\n",
    "    knn = KNeighborsClassifier(n_neighbors=1)\n",
    "    \n",
    "    # 5-fold CV\n",
    "    kf  = KFold(n_splits=5, shuffle = True)\n",
    "    score_i = []\n",
    "    \n",
    "    for train, validation in kf.split(Xs):\n",
    "        knn.fit(Xs[train,:],y[train])\n",
    "        accuracy = knn.score(Xs[validation,:],y[validation])\n",
    "        score_i.append(accuracy) \n",
    "    \n",
    "    score.append(np.mean(score_i))\n",
    "\n",
    "print(\"Error rate (%): \" + str((1-np.mean(score))*100))\n",
    "\n",
    "plt.hist(score)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\">\n",
    "EJERCICIO 3.5: ¿Qué os parece el resultado? ¿Encaja con vuestra intuición? ¿Diríais que es correcto o incorrecto? ¿Por qué?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the RIGHT way\n",
    "\n",
    "np.random.seed(0)\n",
    "score = []\n",
    "\n",
    "for i in range(500): # This runs for a number of experiments. Montecarlo simulation\n",
    "    \n",
    "    # Create toy example\n",
    "    N = 50\n",
    "    y = np.concatenate([-1*np.ones(int(N/2)),np.ones(int(N/2))]) # target\n",
    "    X = np.random.randn(N, 5000)                       # predictors, again random variables\n",
    "    \n",
    "    # 1-neighbor classifier\n",
    "    knn = KNeighborsClassifier(n_neighbors=1)\n",
    "    \n",
    "    # 5-fold CV\n",
    "    kf  = KFold(n_splits=5, shuffle = True)\n",
    "    score_i = []\n",
    "    \n",
    "    for train, validation in kf.split(X):\n",
    "        \n",
    "        # Note here, the ranking and selection is performed inside the CV loop\n",
    "        f_test, _ = f_classif(X[train,:], y[train])\n",
    "        f_test /= np.max(f_test)\n",
    "        ranking = np.argsort(f_test)[::-1] \n",
    "        selected = ranking[0:50]\n",
    "        \n",
    "        Xs = X[:,selected]\n",
    "        \n",
    "        knn.fit(Xs[train,:],y[train])\n",
    "        accuracy = knn.score(Xs[validation,:],y[validation])\n",
    "        \n",
    "        score_i.append(accuracy) \n",
    "    \n",
    "    score.append(np.mean(score_i))\n",
    "\n",
    "print(\"Error rate (%): \" + str((1-np.mean(score))*100))\n",
    "\n",
    "plt.hist(score)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El problema del primer enfoque es que la selección de características se realiza antes del bucle de validación cruzada en lugar de dentro de él. Esto significa que se utiliza información de todo el conjunto de datos para seleccionar las características, y ¿qué provoca esto? en efecto: data leakage. El data leakage (o contaminación de los datos) ocurre cuando durante el proceso de entrenamiento se utiliza información que no estaría disponible en el momento de la predicción, lo que lleva a estimaciones de rendimiento excesivamente optimistas.\n",
    "\n",
    "Vamos: la misma razón por la cual no podemos realizar un análisis/preprocesamiento antes de dividir en train y test.\n",
    "\n",
    "El enfoque correcto es incluir el paso de selección de características dentro del bucle de validación cruzada, de modo que para cada fold, la selección de características se realice utilizando sólo los datos de entrenamiento. Esto asegura que los datos de validación permanezcan ocultos y no influyan en el modelo o en el proceso de selección de características, proporcionando una estimación más precisa del rendimiento del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Métodos embedded\n",
    "\n",
    "Vamos a trabajar directamente sobre la base de datos  de viviendas [House Sales in King County, USA](https://www.kaggle.com/harlfoxem/housesalesprediction), así que lo primero es cargar los datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cargamos datos\n",
    "house_data = pd.read_csv(\"./data/kc_house_data.csv\") # cargamos fichero\n",
    "\n",
    "# Eliminamos las columnas id y date \n",
    "house_data = house_data.drop(['id','date'], axis=1)\n",
    "\n",
    "# convertir las variables en pies al cuadrado en metros al cuadrado \n",
    "feetFeatures = ['sqft_living','sqft_lot','sqft_above','sqft_basement','sqft_living15','sqft_lot15']\n",
    "house_data[feetFeatures] = house_data[feetFeatures].apply(lambda x: x * 0.3048 * 0.3048)\n",
    "\n",
    "# renombramos\n",
    "house_data.columns = ['price','bedrooms','bathrooms','sqm_living','sqm_lot','floors','waterfront','view','condition',\n",
    "                      'grade','sqm_above','sqm_basement','yr_built','yr_renovated','zip_code','lat','long',\n",
    "                      'sqm_living15','sqm_lot15']\n",
    "\n",
    "# añadimos las nuevas variables\n",
    "house_data['years']            = 2015 - house_data['yr_built']\n",
    "#house_data['bedrooms_squared'] = house_data['bedrooms'].apply(lambda x: x**2)\n",
    "#house_data['bed_bath_rooms']   = house_data['bedrooms']*house_data['bathrooms']\n",
    "#house_data['log_sqm_living']   = house_data['sqm_living'].apply(lambda x: np.log(x))\n",
    "#house_data['lat_plus_long']    = house_data['lat']*house_data['long']\n",
    "\n",
    "house_data.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convertimos el DataFrame al formato necesario para scikit-learn\n",
    "data = house_data.values\n",
    "\n",
    "y = data[:,0:1]     # nos quedamos con la 1ª columna, price\n",
    "X = data[:,1:]      # nos quedamos con el resto\n",
    "\n",
    "feature_names = house_data.columns[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a aprovechar para usar un normalizador: [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Dividimos los datos en entrenamiento y test (80 training, 20 test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.25, random_state = 2)\n",
    "\n",
    "print('Datos entrenamiento: ', X_train.shape)\n",
    "print('Datos test: ', X_test.shape)\n",
    "\n",
    "# Escalamos (con los datos de train)\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "XtrainScaled = scaler.transform(X_train)\n",
    "XtestScaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y vamos a aprovechar la capacidad de Lasso para seleccionar variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "alpha_vector = np.logspace(-1,10,20) # Podéis subir esto a 50, yo lo dejo en 20 para que tarde menos\n",
    "param_grid = {'alpha': alpha_vector }\n",
    "grid = GridSearchCV(Lasso(), scoring= 'neg_mean_squared_error', param_grid=param_grid, cv = 10, verbose=2)\n",
    "grid.fit(XtrainScaled, y_train)\n",
    "print(\"best mean cross-validation score: {:.3f}\".format(grid.best_score_))\n",
    "print(\"best parameters: {}\".format(grid.best_params_))\n",
    "\n",
    "#-1 porque es negado\n",
    "scores = -1*np.array(grid.cv_results_['mean_test_score'])\n",
    "plt.semilogx(alpha_vector,scores,'-o')\n",
    "plt.xlabel('alpha',fontsize=16)\n",
    "plt.ylabel('5-Fold MSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "alpha_optimo = grid.best_params_['alpha']\n",
    "lasso = Lasso(alpha = alpha_optimo).fit(XtrainScaled,y_train)\n",
    "\n",
    "ytrainLasso = lasso.predict(XtrainScaled)\n",
    "ytestLasso  = lasso.predict(XtestScaled)\n",
    "mseTrainModelLasso = mean_squared_error(y_train,ytrainLasso)\n",
    "mseTestModelLasso = mean_squared_error(y_test,ytestLasso)\n",
    "\n",
    "print('MSE Modelo Lasso (train): %0.3g' % mseTrainModelLasso)\n",
    "print('MSE Modelo Lasso (test) : %0.3g' % mseTestModelLasso)\n",
    "\n",
    "print('RMSE Modelo Lasso (train): %0.3g' % np.sqrt(mseTrainModelLasso))\n",
    "print('RMSE Modelo Lasso (test) : %0.3g' % np.sqrt(mseTestModelLasso))\n",
    "\n",
    "w = lasso.coef_\n",
    "for f,wi in zip(feature_names,w):\n",
    "    print(f,wi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como puede apreciarse, con este algoritmo hemos conseguido \"desactivar\" algunas variables (*sqm_lot*, *sqm_basement*, *long*, *bedrooms_squared*, *bed_bath_rooms*), lo que mejora la interpretabilidad del modelo a costa de aumentar ligeramente el error en test.\n",
    "\n",
    "No obstante, siguen apareciendo algunas incoherencias con respecto al valor de los coeficientes, como por ejemplo el asociado a la variable *bedrooms* que tiene valor negativo. Si aumentamos el parámetro de regularización y observamos los resultados obtenidos hemos aumentado el error, pero a cambio:\n",
    "\n",
    "1. Tenemos un modelo más sencillo (menos variables al haber muchos coeficientes nulos) y por tanto, menos susceptible a sufrir overfitting\n",
    "2. Mejoramos la interpretabilidad del modelo, las variables supervivientes (con coeficiente distintos de cero) parecen concordar con nuestra intuición sobre problema a resolver\n",
    "\n",
    "Y una vez llegados a este punto, ¿qué podemos hacer? El error de entrenamiento/validación y test es similar, pero todavía es muy alto, así que: \n",
    "\n",
    "1. Se podrían definir nuevas variables que nos ayuden a mejorar el error de predicción (mse)\n",
    "2. Modificar el parámetro de regularización, para mantener el compromiso entre sencillez/interpretabilidad del modelo y error de predicción (MSE)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
